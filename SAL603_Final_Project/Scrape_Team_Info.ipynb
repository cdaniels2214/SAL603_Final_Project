{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e123726-01fe-4d6b-ba9a-e73525b4ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Year: 2024 - URL: https://cfbstats.com/2024/team/index.html\n",
      "\n",
      "Processing Year: 2023 - URL: https://cfbstats.com/2023/team/index.html\n",
      "\n",
      "Processing Year: 2022 - URL: https://cfbstats.com/2022/team/index.html\n",
      "Saved 134 teams for 2024\n",
      "Saved 131 teams for 2022\n",
      "Saved 133 teams for 2023\n",
      "\n",
      "Scraping complete. Data saved to C:\\Users\\Christopher\\OneDrive - Syracuse University\\PythonSportAnalytics\\Section_8\\Final_Project\\CSV_Files\\cfbstats_teams_2022_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define output file\n",
    "base_dir = r\"C:\\Users\\Christopher\\OneDrive - Syracuse University\\PythonSportAnalytics\\Section_8\\Final_Project\\CSV_Files\"\n",
    "output_file = os.path.join(base_dir, \"cfbstats_teams_2022_2024.csv\")\n",
    "\n",
    "# Ensure the output file exists with proper headers\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=[\"Team\", \"Year\", \"Team ID\", \"Team URL\"]).to_csv(output_file, index=False)\n",
    "\n",
    "# Base URLs for scraping\n",
    "base_urls = {\n",
    "    2024: \"https://cfbstats.com/2024/team/index.html\",\n",
    "    2023: \"https://cfbstats.com/2023/team/index.html\",\n",
    "    2022: \"https://cfbstats.com/2022/team/index.html\"\n",
    "}\n",
    "\n",
    "# User-Agent list and Referers\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "REFERERS = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.bing.com/\",\n",
    "    \"https://www.yahoo.com/\",\n",
    "    \"https://duckduckgo.com/\"\n",
    "]\n",
    "\n",
    "# Track scraped teams to avoid duplicates\n",
    "scraped_teams = set()\n",
    "\n",
    "# Function to scrape a single year's teams\n",
    "def scrape_teams(year, url):\n",
    "    print(f\"\\nProcessing Year: {year} - URL: {url}\")\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(USER_AGENTS),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": random.choice(REFERERS),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch {url} for {year} (Error: {e})\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    team_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    teams_data = []\n",
    "    for link in team_links:\n",
    "        team_url = link[\"href\"]\n",
    "\n",
    "        # Extract only valid team links: '/team/{id}/index.html' (ignore \"/team/index.html\")\n",
    "        if \"/team/\" in team_url and \"/index.html\" in team_url and not team_url.endswith(\"/team/index.html\"):\n",
    "            team_name = link.text.strip()\n",
    "\n",
    "            # Ensure it's not mistakenly pulling a year as a team name\n",
    "            if team_name.isdigit():\n",
    "                continue  # Skip bad data like \"2024\", \"2023\", etc.\n",
    "\n",
    "            match = re.search(r\"/team/(\\d+)/index\\.html\", team_url)\n",
    "            team_id = match.group(1) if match else None  # Correct extraction\n",
    "\n",
    "            if not team_id:\n",
    "                print(f\"Warning: Could not extract Team ID from {team_url}\")\n",
    "                continue\n",
    "\n",
    "            full_link = f\"https://cfbstats.com{team_url}\"\n",
    "\n",
    "            # Skip duplicates\n",
    "            if (team_name, year) in scraped_teams:\n",
    "                print(f\"Skipping duplicate: {team_name} ({year})\")\n",
    "                continue\n",
    "\n",
    "            teams_data.append({\"Team\": team_name, \"Year\": year, \"Team ID\": team_id, \"Team URL\": full_link})\n",
    "            scraped_teams.add((team_name, year))\n",
    "\n",
    "    # Save valid data\n",
    "    if teams_data:\n",
    "        df = pd.DataFrame(teams_data)\n",
    "        df.to_csv(output_file, mode=\"a\", header=not os.path.exists(output_file), index=False)\n",
    "        print(f\"Saved {len(teams_data)} teams for {year}\")\n",
    "\n",
    "    return teams_data\n",
    "\n",
    "# Use ThreadPoolExecutor to scrape multiple years\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    future_to_year = {executor.submit(scrape_teams, year, url): year for year, url in base_urls.items()}\n",
    "\n",
    "    for future in as_completed(future_to_year):\n",
    "        year = future_to_year[future]\n",
    "        try:\n",
    "            future.result()  # Ensure exceptions are raised if they occur\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {year}: {e}\")\n",
    "\n",
    "print(f\"\\nScraping complete. Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b96be8bd-cbcd-4365-9fd2-887262b31379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification passed: Each team has exactly one entry per year.\n",
      "\n",
      "All teams from the reference file are present in the main file.\n",
      "\n",
      "All teams from the main file are present in the reference file.\n",
      "\n",
      "Updated file saved at: C:\\Users\\Christopher\\OneDrive - Syracuse University\\PythonSportAnalytics\\Section_8\\Final_Project\\CSV_Files\\cfbstats_teams_2022_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# Error checking and file data verification\n",
    "# Define file paths\n",
    "base_dir = r\"C:\\Users\\Christopher\\OneDrive - Syracuse University\\PythonSportAnalytics\\Section_8\\Final_Project\\CSV_Files\"\n",
    "output_file = os.path.join(base_dir, \"cfbstats_teams_2022_2024.csv\")\n",
    "reference_file = os.path.join(base_dir, \"fbs_teams_links.csv\")  # Reference file for verification\n",
    "\n",
    "# Load both CSV files\n",
    "df_main = pd.read_csv(output_file)\n",
    "df_ref = pd.read_csv(reference_file)\n",
    "\n",
    "# *ADDED LATER* Remove unwanted teams, I scraped teams I didn't need because original code was wrong\n",
    "#teams_to_remove = {\"add team needing removed here\"}\n",
    "#df_main = df_main[~df_main[\"Team\"].isin(teams_to_remove)]\n",
    "\n",
    "# Verify that each unique team has ONE instance per year\n",
    "team_year_counts = df_main.groupby([\"Team\", \"Year\"]).size().reset_index(name=\"Count\")\n",
    "duplicates = team_year_counts[team_year_counts[\"Count\"] > 1]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(\"\\nWARNING: The following teams have multiple entries for a single year:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"\\nVerification passed: Each team has exactly one entry per year.\")\n",
    "\n",
    "# Extract unique team names for comparison\n",
    "teams_main = set(df_main[\"Team\"])\n",
    "teams_ref = set(df_ref[\"School\"])\n",
    "\n",
    "# Find discrepancies, if any, in the files created\n",
    "missing_in_main = teams_ref - teams_main  # Teams in reference but missing in main\n",
    "missing_in_ref = teams_main - teams_ref  # Teams in main but missing in reference\n",
    "\n",
    "# Print results\n",
    "if missing_in_main:\n",
    "    print(\"\\nTeams present in reference file but MISSING in main file:\")\n",
    "    print(\"\\n\".join(missing_in_main))\n",
    "else:\n",
    "    print(\"\\nAll teams from the reference file are present in the main file.\")\n",
    "\n",
    "if missing_in_ref:\n",
    "    print(\"\\nTeams present in main file but MISSING in reference file:\")\n",
    "    print(\"\\n\".join(missing_in_ref))\n",
    "else:\n",
    "    print(\"\\nAll teams from the main file are present in the reference file.\")\n",
    "\n",
    "# Sort the file alphabetically by team name, then by year (descending)\n",
    "df_main = df_main.sort_values(by=[\"Team\", \"Year\"], ascending=[True, False])\n",
    "\n",
    "# Save cleaned and sorted file\n",
    "df_main.to_csv(output_file, index=False)\n",
    "print(f\"\\nUpdated file saved at: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b2d9c90-2ee5-4e11-b34e-0063bf2c8f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique teams in main file: 127\n",
      "Total number of unique teams in reference file: 127\n",
      "\n",
      "All teams from the reference file are present in the main file.\n",
      "\n",
      "All teams from the main file are present in the reference file.\n",
      "\n",
      "Updated file saved at: C:\\Users\\Christopher\\OneDrive - Syracuse University\\PythonSportAnalytics\\Section_8\\Final_Project\\CSV_Files\\cfbstats_teams_2022_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# Second Verification of all teams, counts, and any potential missing or additional teams we don't want becuase I made mistakes above\n",
    "output_file = os.path.join(base_dir, \"cfbstats_teams_2022_2024.csv\")\n",
    "reference_file = os.path.join(base_dir, \"fbs_teams_links.csv\")  # Reference file for verification\n",
    "\n",
    "# Load both CSV files\n",
    "df_main = pd.read_csv(output_file)\n",
    "df_ref = pd.read_csv(reference_file)\n",
    "\n",
    "# Count the total number of unique teams in both CSV files\n",
    "total_teams_main = df_main[\"Team\"].nunique()\n",
    "total_teams_ref = df_ref[\"School\"].nunique()\n",
    "\n",
    "# Print the total number of unique teams in each file\n",
    "print(f\"Total number of unique teams in main file: {total_teams_main}\")\n",
    "print(f\"Total number of unique teams in reference file: {total_teams_ref}\")\n",
    "\n",
    "# Extract unique team names for comparison\n",
    "teams_main = set(df_main[\"Team\"])\n",
    "teams_ref = set(df_ref[\"School\"])\n",
    "\n",
    "# Apply the name corrections (teams were not named the same from the different sites, but only some, so this allowed me to change them manually)\n",
    "df_main[\"Team\"] = df_main[\"Team\"].replace(name_corrections)\n",
    "df_ref[\"School\"] = df_ref[\"School\"].replace(name_corrections)\n",
    "\n",
    "# Find discrepancies (teams missing in one file or another)\n",
    "missing_in_main = teams_ref - set(df_main[\"Team\"])\n",
    "missing_in_ref = teams_main - set(df_ref[\"School\"])\n",
    "\n",
    "# Print results\n",
    "if missing_in_main:\n",
    "    print(\"\\nTeams present in reference file but MISSING in main file (after name corrections):\")\n",
    "    print(\"\\n\".join(missing_in_main))\n",
    "else:\n",
    "    print(\"\\nAll teams from the reference file are present in the main file.\")\n",
    "\n",
    "if missing_in_ref:\n",
    "    print(\"\\nTeams present in main file but MISSING in reference file (after name corrections):\")\n",
    "    print(\"\\n\".join(missing_in_ref))\n",
    "else:\n",
    "    print(\"\\nAll teams from the main file are present in the reference file.\")\n",
    "\n",
    "# Sort the file alphabetically by team name, then by year (descending)\n",
    "df_main = df_main.sort_values(by=[\"Team\", \"Year\"], ascending=[True, False])\n",
    "\n",
    "# Save cleaned and sorted file\n",
    "df_main.to_csv(output_file, index=False)\n",
    "print(f\"\\nUpdated file saved at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bff89a-f202-4fac-ace9-5d24afb8d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
